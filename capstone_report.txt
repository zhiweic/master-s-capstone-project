KIVA MICROLOANS CAPSTONE PROJECT FINAL REPORT
Research question statement
Will a loan be fully funded?

KIVA Microloans is a non-profit organization that runs an “online career support and empowerment program”. It allows its users to connect through lending money to other people (mainly entrepreneurs and students) in over 70 countries. It is becoming a helpful Internet application that provides safe and accessible capital support to the borrowers and a quick and easy way for the loaners to invest their kindness. On its website, one can search for loans by categories, sectors, and other attributes of the borrowers and read the story of the loan, the repayment schedule and possibly profiles of other lenders who contributed.  It is a very handy product for borrowers to fundraise overseas in all kinds of job fields and social roles. For this capstone project, I will be analyzing the Microloans data to give some insights on the lender’s behavior and answering the questions: can we predict if a loan will be fully funded? What kinds of loans are likely to be funded? 

The data I am using for this project is an archived public JSON snapshot from the Kiva API. It contains more than 1.2 million lenders information and more than 650 thousands of loans. The raw data lives in thousands of JSON format files generated from hundreds or thousands of requests to the API. The API data has three objects, namely the lender object, the loans object and the loans_lenders object that connects the two main objects. Each object has attributes saved in a key-value pair dictionary data type. For this project I am only using the loans object data of the raw dataset. Since the original dataset has more than 650,000 entries, and not all of the data fields are relevant to answering my research questions, it needs a lot of pre-processing. 

The data fields retrieved from the raw JSON files include numerical fields: funded amount, journal entries, lender count, paid amount, loan amount, categorical fields: gender, sector, country, town, activity, id, status, first name, pictured and free user input field: description. The preprocessing mainly has 5 steps: pulling data values out of the JSON dictionary and unlisting the items, encoding the text fields from Unicode, exporting relevant variables into a easy-to-read .csv file, loading data into a panda data frame, and filtering and filling in missing information.

After cleaning everything up into a usable data frame, I did some exploratory data analysis by looking at some summary statistics and identifying trends in how some of the variables are correlated. My first interesting discovery is that 20% of the countries (about 15 of them) own 430570 loans (more than 70% of total loans), which agrees with the 80-20 rule saying that roughly 80% of the effects come from 20% of the causes. This is a classical rule reflecting the distribution of capital and wealth. Even in this mini-scale Internet trading environment, we can see that the cash flows follow the rule. The top five countries with the most number of loans are Philippines, which has twice as many loans as the second place, Peru, followed by Kenya, Cambodia and Nicaragua.

Another intuition to check is if loans with higher funded amount have larger lenders count. I plotted the scatter plot with funded amount on the y-axis and lender count on the x-axis, and then fitted a regression line to the data points. The slope of the regression line is significantly positive, and it tells us that each additional lender of a loan contributes to around $30. However, from the scatterplot we observe that this is only true when the funded amount is big. It’s pretty clear that once the funded amount hit 10,000, the more money funded the more lenders needed to achieve that. With loans lower than $10,000, we cannot identify a pattern. Also it is interesting to see which sectors have the most number of loans, and later on I would want to further discuss if the sector of a loan affects its probability of being fully funded. From the crosstab of sector and loan count, we see that the top three sectors are Food, Retail and Agriculture. Philippines, the country that owns the most number of loans, also owns the most loans in these three sectors. To see if loan amount differs by sectors, I looked at a side-by-side boxplot of all the sectors. Since we have relatively few loans more than $10,000, those over are all treated as outliers, and we cannot really see what is going on with the smaller loans. Therefore, I decided to focus on the mini-loans of amount less than or equal to $2000. I also observed that the loans in the top three sectors are three out of five of sectors with the least average loan amount. Wholesale, entertainment and health are the top three sectors with the largest average loan amount.

The next step is to clean up the data that I wanted to focus on doing more advance analysis. I filtered out all the loans higher than $2000, and those with status “refunded” and “issue”. These two statuses mean that there were some exception and unexpected situation with the loans, so I would want to clear these noises from my dataset. With the clean data, I again looked at the relationship between sector and loan amount. From table, we can see that now wholesale, health and education are the top three sectors with the most expensive average loans. Comparing to my previous results, entertainment is kicked out of the top 5, which indicates that there are some loans in the entertainment sector with a huge loan amount that are now filtered out as “outliers”. The bottom three sectors are now food, retail and arts.

Whenever a lender browses through the website, the first thing that he/she will look at is most probably going to be the description of the loan. Then naturally we would want to know if the description of the borrowers tells us anything about the likeliness of a loan being funded. To do this, I focus on only the description and loan status fields and trying to classify loans based on the text information. Loans with status “fundraising” are not included for training my model, since it is unclear if they will be funded or not. In other words, the usage of my classifier is to help predict whether a fundraising loan will be fully funded. I recoded the loan status as either fully funded (1) or not fully funded (0) as following:
 
Status	Paid	In repayment	Funded	Defaulted	Inactive expired	Expired	Deleted	Inactive
Recode	1	1	1	1	0	0	0	0


After recoding the statuses, I have more than 580 thousands of fully funded loans, and 18.7 thousands of not fully funded ones, which is only about 3.2% of the funded ones. This imbalance of 0/1 cases might result in some over-fitting of under-fitting problems that will be discussed later.

In order to recognize the patterns in descriptions, I decided to use statistical classifiers to capture the features in the text. A Naïve Bayes Classifier is the most desired method. Some advantages for the Naïve Bayes Classifiers are: 1) it is a very simple probabilistic classifier based on the Bayes rule; 2) it can be trained very efficiently in a supervised learning setting. Usually the strong independence assumptions it requires are not true. However, the multinomial Naïve Bayes Classifier is suitable for classification with discrete features (e.g., word counts for text classification). In other words, for the description I have, the presence of a particular word is usually independent of any other words. Some exceptions are when the author uses some common phrases, which is taken into consideration and will be solved later in the process. The descriptions can be viewed as a sample from the “bag-of-words model”, which is represented by a bag of words, disregarding grammar or word order to keep the simplicity. This model is commonly used for natural language processing and document classification. Since word order and meaning cannot be quantified easily, the frequency of each word is used as a key feature for training a classifier. In my first step of determining which classifier to use, I evaluated performance of the simple word count weighting and the tf-idf (term frequency–inverse document frequency) weighting method with 5-fold cross validation. Here I used the sklearn and scipy stats module. The main difference of the tf-idf value compared to the usual count weighting is that it controls for the fact that some words generally occur more often than others by increasing the measure proportionally to the number of times a word appears in the paragraph but also downscaling by its frequency in other paragraphs.

From the cross validation results, we were able to tell that the tf-idf weighting did a superior job. Tf-idf can be used for stop-words filtering with its special weighting scheme. To ensure that the common words and phrases were thrown out, I decided to impose a list of stop-word to the classifier so that these words and phrases will be filtered out prior to the text processing of descriptions. Again, with 5-fold cross validation, results showed that stop-words effectively improved the performance of the classifier.

Another possible change to the classifier has to do with the fact that a multinomial distribution deals with categories. Applying an additive smoothing technique to the classifier permits the assignment of non-zero probabilities to words not in the sample. Additive smoothing, a.k.a Lidstone smoothing is commonly a component of naïve Bayes classifiers. Although this did not improve the cross validation result, it could be help for training the data, since we have limited data on unfunded loans. With additive smoothing, we will be able to have a more complete word frequency list. For all of these text classifiers, I used a pipeline to combine the process of weighting the words and cross validating.

Now that I have two candidates for my classifier, I could train and evaluate them with my built-up pipeline. The sklearn module gives a very nicely formatted metric report on classification, which came in to be handy for classifier evaluation. In the classification metrics report, precision (1) = true positive/(true positive + false positive), recall (1) = true positive/(true positive+ false negative), f1-score (1), harmonic mean of precision and recall, = 2tp/(2tp+fp+fn). In both of the reports, the precision rate, recall rate and f1-score for funded loans are very high, around 0.97-0.99. This tells us that we are doing very well in predicting the status of the funded ones. However, when we look at the precision, recall and f1-score for the not fully funded loans, the metrics are a little disappointing. For precision (0), we see that only 40-50% of the unfunded loans are correctly classified. While for recall (0), even with Lidstone smoothing, 90% of the loans predicted to be unfunded are actually fully funded. Since I want to give positive feedbacks for as much cases as possible, i.e. I don’t want the funded loans to be misclassified, I chose the classifier with Lidstone smoothing and sacrificing the precision for a higher recall for unfunded loans. Although the testing and training sets have very high classification score, it does not mean that our classifier is doing excellent. As discussed earlier, we have much more funded loans than unfunded ones, therefore the classifier has more information to predict the funded ones and thus does better in precision (1) and recall (1). 

To be clearer on how the classifier is doing in classifying the loans, I plotted the ROC (Receiver Operating Characteristic) curve. The area under the curve is the overall accuracy of my classifier, about 75.7%.  There are two limitations in my data and model. 1) I have a huge imbalance in information on funded and unfunded loans; 2) Prediction based on only the description field could be presumably not enough data.

To make a better prediction, I then looked at the demographic information in the dataset, and chose the following variables to enter my logistic regression model: loan amount, journal entries, sector, gender and pictured. The pictured field is a Boolean variable with True meaning the user has a profile picture and False otherwise.

The logistics regression results tells me that an average male is less likely to have a fully funded loan than a similar female with log odd around 0.59. Having more journal entries increases the probability of a loan getting funded by a lot, while having a profile picture does not really help the borrower at all. Surprisingly, having a larger loan amount only lowers the probability of getting funded by a small margin, less than the effect of journal entries.

This scatterplot of journal entries and loan amount color coded with red as not funded and clue as funded shows that it is almost unlikely to have an unfunded loan if a user has more than 2 journal entries.

As for sectors, education, entertainment and manufacturing are the three sectors that are funded most often, while housing, personal use and clothing are the least funded. Education, even being one of the larger loan sectors, still gets funded very often. Entertainment and manufacturing also have relatively more expensive loans but at the same time are funded more often as well. The least funded sectors do not have the most expensive or the cheapest loan amounts, and the uses are more personal than job oriented. This confirms the intuition that people lending using KIVA cares more about the motivation and inspiration of the borrowers and less about the actual amount of money.
